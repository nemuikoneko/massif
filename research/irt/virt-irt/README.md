Here I explore the idea from  [How many words do children know?](https://www.researchgate.net/profile/Sascha-Schroeder-5/publication/301693773_How_many_words_do_children_know_A_corpus-based_estimation_of_children%27s_total_vocabulary_size/links/5c7ed620299bf1268d3ccbf3/How-many-words-do-children-know-A-corpus-based-estimation-of-childrens-total-vocabulary-size.pdf?origin=publication_detail) of simulating "virtual" person's whose knowledge of specific words is determined by random selection based on word frequency statistics. I'm taking the idea even more virtual by using synthetic word statistics, namely the simple Zipf's law that the frequency of a word is proportional to 1/rank. The goal is to compute IRT parameters for the virtual word-items, and see if I can use regression to come up with a simple formula to directly compute IRT parameters from rank that would apply _decently_ well to "proficiency testing" over _any_ word ordering.

We use the [py-irt](https://github.com/nd-ball/py-irt) tool to compute IRT parameters of virtual word-items, which is overkill given the small size of the data set, but convenient.

Once dependencies are installed (see `../README.md`), this can be run with `./run.sh`.

My takeaway was that, even varying the "threshold" (second parameter of `pick_random_lexicon_thresh`), the computed word-item difficulty was pretty close to the _log_ of the word rank, not the word rank as suggested elsewhere. I suspect that this simulation should under-score the difficulty of higher-rank words, because it assumes that just encountering a word N times is enough to know it (it doesn't take into account the longer gaps between seeing rarer words, how that makes them harder to remember). If rarer words were harder, that should make the difficulty-vs-rank curve more linear-shaped than log-shaped.
